{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8386fc4d",
   "metadata": {},
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e3e2f",
   "metadata": {},
   "source": [
    "[Friedman, 2001]\n",
    "\n",
    "$$ \\sum_{i=1}^n L(y_i,f(x_i))\\to\\min_{f\\in\\mathscr H}$$\n",
    "\n",
    "Idea: look for the approximate solution of the form\n",
    "\n",
    "$$ f=\\sum_{i=1}^m \\alpha_i h(x_i),\\quad h\\in\\mathscr H,\\quad \\alpha_i>0, $$\n",
    "\n",
    "and try obtain this solution recursively:\n",
    "\n",
    "$$ f_k=f_{k-1}+\\alpha_k h_k. $$\n",
    "\n",
    "If $\\hat f_{k-1}$ is already constructed, then\n",
    "\n",
    "$$\\hat h_k\\in\\arg\\min_{\\alpha\\ge 0,\\\\ h\\in\\mathscr H}\\sum_{i=1}^n L(y_i,\\hat f_{k-1}(x_i)+\\alpha h(x_i)).$$\n",
    "\n",
    "Instead of getting the exact solution, we want to make one step of the gradient method in $\\mathbb R^n$ from the point $\\hat f_{k-1}(x_i)$ in the direction of $\\bigl(-L_{\\hat y}(y_i,\\hat f_{k-1}(x_i))\\bigr)_{i=1}^n$. \n",
    "In fact, we first approximate this vector by some $h\\in\\mathcal H$, which solves the OLS problem:\n",
    "\n",
    "$$\\hat h_k\\in\\arg\\min_{h\\in\\mathcal H} \\sum_{i=1}^n\\bigl(L_{\\hat y}(y_i,\\hat f_{k-1}(x_i))+h(x_i)\\bigr)^2 \\tag{1}$$\n",
    "\n",
    "and then make the step in the function space $\\mathscr H$:\n",
    "\n",
    "$$ \\hat f_k:=\\hat f_{k-1}+\\hat \\alpha_k \\hat h_k \\tag{2} $$\n",
    "\n",
    "where the step size $\\widehat\\alpha_k$ is obtained from the one-dimensional optimization problem\n",
    "\n",
    "$$ \\hat\\alpha_k\\in\\arg\\min_{\\alpha\\ge 0}\\sum_{i=1}^n L(y_i,\\hat f_{k-1}(x_i)+\\alpha \\hat h_k(x_i)). \\tag{3}$$\n",
    "\n",
    "For an arbitrary intial guess $\\hat f_0$ the general gradient boosting algorithm is defined by (1), (3), (2):\n",
    "\n",
    "* (1): approximate current anti-gradient\n",
    "* (3): find the best step size by the line search\n",
    "* (2): make the step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a2d3c",
   "metadata": {},
   "source": [
    "#### Least-squares regression: $L(y,\\hat y)=(y-\\hat y)^2/2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37123430",
   "metadata": {},
   "source": [
    "Since $L_{\\hat y}=2(\\hat y -y)$, we have\n",
    "\n",
    "$$\\hat h_k\\in\\arg\\min_{h\\in\\mathcal H} \\sum_{i=1}^n\\bigl(y_i-f_{k-1}(x_i)-h(x_i)\\bigr)^2$$\n",
    "\n",
    "Thus, at each step the model is refitted to current residuals. The step size:\n",
    "\n",
    "$$ \\hat\\alpha_k\\in\\arg\\min_{\\alpha\\ge 0}\\sum_{i=1}^n \\bigl(y_i-f_{k-1}(x_i)-\\alpha h_k(x_i)\\bigr)^2=\\frac{\\sum_{i=1}^n(y_i-f_{k-1}(x_i))h_k(x_i)}{\\sum_{i=1}^n h_k^2(x_i)}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f552833b",
   "metadata": {},
   "source": [
    "#### AdaBoost: $L(y,\\hat y)=e^{-y\\hat y}$, $y\\in \\mathcal Y=\\{-1,1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d3b5b",
   "metadata": {},
   "source": [
    "[Freund, Schapire, 1995]\n",
    "\n",
    "The base class $\\mathscr H$ contains contains some set of classifiers $h:\\mathcal X\\to \\{-1,1\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a31ebd",
   "metadata": {},
   "source": [
    "Since $L_{\\hat y}(y,\\hat y)=-ye^{-y\\hat y}$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "&\\arg\\min_{h\\in\\mathcal H} \\sum_{i=1}^n\\bigl(L_{\\hat y}(y_i,\\hat f_{k-1}(x_i))+h(x_i)\\bigr)^2=\\arg\\min_{h\\in\\mathcal H}\\sum_{i=1}^n\\bigl(-y_i e^{-y_i f_{k-1}(x_i)}  +h(x_i)\\bigr)^2\\\\\n",
    "&=\\arg\\min_{h\\in\\mathcal H}\\sum_{i=1}^n(-2 y_i h(x_i) e^{-y_i f_{k-1}(x_i)}+h(x_i)^2)=\\arg\\max_{h\\in\\mathcal H}\\sum_{i=1}^n y_i h(x_i) e^{-y_i f_{k-1}(x_i)}\\\\\n",
    "&=\\arg\\max_{h\\in\\mathcal H}\\left(\\sum_{h(x_i)=y_i}e^{-y_i f_{k-1}(x_i)}-\\sum_{h(x_i)\\neq y_i}e^{-y_i f_{k-1}(x_i)}\\right)=\\arg\\max_{h\\in\\mathcal H}\\left(\\sum_{i=1}^ne^{-y_i f_{k-1}(x_i)}-2\\sum_{h(x_i)\\neq y_i}e^{-y_i f_{k-1}(x_i)}\\right)\\\\\n",
    "&=\\arg\\min_{h\\in\\mathscr H}\\sum_{h(x_i)\\neq y_i}e^{-y_i f_{k-1}(x_i)}=\\arg\\min_{h\\in\\mathscr H}\\sum_{i=1}^n w_i I(h(x_i)\\neq y_i),\\qquad w_i:=\\frac{e^{-y_i f_{k-1}(x_i)}}{\\sum_{j=1}^n e^{-y_j f_{k-1}(x_j)}}.\n",
    "\\end{align*}\n",
    "\n",
    "The optimal classifier tries to correctly classify the points with large weights $e^{-y_i f_{k-1}(x_i)}$. In fact those which were incorrectly classified by $f_{k-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d48ba6",
   "metadata": {},
   "source": [
    "*Optimal stepsize*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9af2a7",
   "metadata": {},
   "source": [
    "$$ \\hat\\alpha_k\\in\\arg\\min_{\\alpha\\ge 0}\\sum_{i=1}^n L(y_i,\\hat f_{k-1}(x_i)+\\alpha \\hat h_k(x_i))=\\min_{\\alpha\\ge 0}\\sum_{i=1}^n e^{-y_i(f_k(x_i)+\\alpha h_k(x_i))}.$$\n",
    "\n",
    "Derivative with respect to $\\alpha$:\n",
    "\n",
    "$$-\\sum_{i=1}^n y_i h_k(x_i) e^{-y_i(f_{k-1}(x_i)+\\alpha h_k(x_i))}=-\\sum_{h_k(x_i)=y_i}e^{-y_if_{k-1}(x_i)} e^{-\\alpha}+\\sum_{h_k(x_i)\\neq y_i}e^{-y_i f_{k-1}(x_i)} e^{\\alpha}=0.$$\n",
    "\n",
    "Eqiivalently, \n",
    "\n",
    "$$\\sum_{h_k(x_i)=y_i} w_i e^{-\\alpha}-\\sum_{h_k(x_i)\\neq y_i} w_i e^{\\alpha}=(1-\\varepsilon_k) e^{-\\alpha}-\\varepsilon_k e^\\alpha=0,\\qquad \\varepsilon_k=\\sum_{h_k(x_i)\\neq y_i} w_i.$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$ \\hat\\alpha_k=\\frac{1}{2}\\ln\\frac{1-\\varepsilon_{k}}{\\varepsilon_{k}}.  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6f698",
   "metadata": {},
   "source": [
    "*Weight update*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3513c05",
   "metadata": {},
   "source": [
    "Let us give the weights additional time index:\n",
    "\n",
    "$$w_{k-1,i}:=\\frac{e^{-y_i f_{k-1}(x_i)}}{\\sum_{j=1}^n e^{-y_j f_{k-1}(x_j)}}$$\n",
    "    \n",
    "Unnormalized weights update:\n",
    "\n",
    "$$ \\widetilde w_{k,i}=e^{-y_i f_k (x_i)}=e^{-y_i f_{k-1} (x_i)}e^{-\\alpha_k y_i h_k (x_i)}=\\widetilde w_{k-1,i}e^{-\\alpha_k y_i h_k (x_i)}. $$\n",
    "\n",
    "Furthermore, put\n",
    "\n",
    "$$ Z_k=\\sum_{j=1}^n \\widetilde w_{k,j}=\\sum_{j=1}^n e^{-y_j f_k (x_j)}. $$\n",
    "\n",
    "We have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{Z_k}{Z_{k-1}} &=\\frac{\\sum_{i=1}^n e^{-y_i(f_{k-1}(x_i)+\\alpha_k h_k(x_i))}}{\\sum_{j=1}^n e^{-y_j f_{k-1}(x_j)}}=\\sum_{i=1}^n w_{k-1,i} e^{-\\alpha_k y_i h_k(x_i)}\\\\\n",
    "&=e^{-\\alpha_k}\\sum_{y_i=h_k(x_i)} w_{k-1,i}+e^{\\alpha_k}\\sum_{y_i\\neq h_k(x_i)} w_{k-1,i}=e^{-\\alpha_k} (1-\\varepsilon_k)+e^{\\alpha_k}\\varepsilon_k\n",
    "\\end{align*}\n",
    "\n",
    "Substituting $\\alpha_k$:\n",
    "\n",
    "$$ e^{-\\alpha_k}=\\sqrt\\frac{\\varepsilon_k}{1-\\varepsilon_k}, \\quad e^{\\alpha_k}=\\sqrt\\frac{1-\\varepsilon_k}{\\varepsilon_k},$$\n",
    "\n",
    "we get \n",
    "\n",
    "$$\\frac{Z_k}{Z_{k-1}}=2\\sqrt{\\varepsilon_k(1-\\varepsilon_k)},$$ \n",
    "\n",
    "$$ w_{k,i}=\\frac{\\widetilde w_{k,i}}{Z_k}=\\frac{\\widetilde w_{k-1,i}e^{-\\alpha_k y_i h_k (x_i)}}{Z_{k-1} 2\\sqrt{\\varepsilon_k(1-\\varepsilon_k)}}=w_{k-1,i}\\frac{e^{-\\alpha_k y_i h_k (x_i)}}{2\\sqrt{\\varepsilon_k(1-\\varepsilon_k)}} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f68072",
   "metadata": {},
   "source": [
    "*AdaBoost algorithm:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f451863",
   "metadata": {},
   "source": [
    "Starting from zero classifier $f_0=0$, and the corresponding uniform weights $w_{0,i}=1$,\n",
    "\n",
    "* Approximate current anti-gradient: that is, find the classifier\n",
    "\n",
    "$$h_k=\\arg\\min_{h\\in\\mathscr H}\\sum_{i=1}^n w_{k-1,i} I(h(x_i)\\neq y_i),\\qquad w_{k-1,i}:=\\frac{e^{-y_i f_{k-1}(x_i)}}{\\sum_{j=1}^n e^{-y_j f_{k-1}(x_j)}}$$\n",
    "\n",
    "* Compute the step size\n",
    "\n",
    "$$ \\hat\\alpha_k=\\frac{1}{2}\\ln\\frac{1-\\varepsilon_{k}}{\\varepsilon_{k}}, \\qquad \\varepsilon_k=\\sum_{h_k(x_i)\\neq y_i} w_{k,i}.  $$\n",
    "\n",
    "* Make the step:\n",
    "\n",
    "$$ f_k:=f_{k-1}+ \\alpha_k  h_k $$\n",
    "\n",
    "and update the weights:\n",
    "\n",
    "$$ w_{k,i}=w_{k-1,i}\\frac{e^{-\\alpha_k y_i h_k (x_i)}}{2\\sqrt{\\varepsilon_k(1-\\varepsilon_k)}}. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d582c2d",
   "metadata": {},
   "source": [
    "**Theorem**. Assume that at each iteration of AdaBoost, the weak learner returns a hypothesis for which $\\varepsilon_t\\le 1/2-\\gamma$. Then, the training error of the output hypothesis of AdaBoost is at most\n",
    "\n",
    "$$ L_S(f_m)=\\frac{1}{n}\\sum_{i=1}^n I_{\\{f_m(x_i)\\neq y_i\\}}\\le e^{-2\\gamma^2 m} $$\n",
    "\n",
    "(see Shalev-Schwartz, Ben-David, 2014)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4402f589",
   "metadata": {},
   "source": [
    "*Proof.*\n",
    "\n",
    "\\begin{align*}\n",
    " L_S(f_m) &=\\frac{1}{n}\\sum_{i=1}^n I_{\\{f_m(x_i)\\neq y_i\\}}\\le \\frac{1}{n}\\sum_{i=1}^n e^{-y_i f_m(x_i)}\\\\\n",
    " &=\\frac{1}{n} Z_m=\\frac{Z_0}{n}\\prod_{k=1}^m 2 \\sqrt{\\varepsilon_k(1-\\varepsilon_k)}=2^m\\prod_{k=1}^m \\sqrt{\\varepsilon_k(1-\\varepsilon_k)}.\n",
    "\\end{align*}\n",
    "\n",
    "The function $g(c)=c(1-c)$ increases at $[0,1/2]$. By our assumption, $\\varepsilon_k\\le 1/2-\\gamma$. Thus,\n",
    "\n",
    "$$ L_S(f_m)\\le 2^m\\prod_{k=1}^m \\sqrt{(1/2-\\gamma)(1/2+\\gamma)}=\\left(1- 4\\gamma^2\\right)^{m/2}. $$\n",
    "\n",
    "From the inequality $1-a\\le e^{-a}$ it follows that\n",
    "\n",
    "$$ \\sqrt{1-4\\gamma^2}\\le e^{-2\\gamma^2}.\\qquad \\square$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb527e46",
   "metadata": {},
   "source": [
    "#### TreeBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d7a208",
   "metadata": {},
   "source": [
    "[Friedman, 2001]\n",
    "\n",
    "Basic class $\\mathscr H$ consists of tree classifiers\n",
    "\n",
    "$$ f(x)=\\sum_{j=1}^J b_j I(x\\in R_j). $$\n",
    "\n",
    "In general optimal step size is obtained as\n",
    "\n",
    "$$ \\alpha_k\\in\\arg\\min_{\\alpha\\ge 0}\\sum_{i=1}^n L(y_i,\\hat f_{k-1}(x_i)+\\alpha \\hat h_k(x_i)). $$\n",
    "\n",
    "The update rule:\n",
    "\n",
    "$$ f_k(x)=f_{k-1}(x)+\\alpha_k \\sum_{j=1}^J b_{k,j} I(x\\in R_{k,j})=f_{k-1}(x)+\\sum_{j=1}^J \\gamma_{k,j} I(x\\in R_{k,j})$$\n",
    "\n",
    "In TreeBoost instead of one optimal $\\alpha_k$ for each classifier one looks for optimal $\\gamma_{k,j}$ separately at each $R_{k,j}$:\n",
    "\n",
    "$$ \\gamma_{kj}=\\arg\\min_{\\gamma_{kj}}\\sum_{i=1}^n L(y_i,\\hat f_{k-1}(x_i)+\\gamma_{kj}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334232f9",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b818e9c8",
   "metadata": {},
   "source": [
    "[Chen, Guestrin 2016]\n",
    "\n",
    "* Take trees as a basic class $\\mathscr H$:\n",
    "\n",
    "$$ f(x)=\\sum_{j=1}^J w_j I(x\\in R_j). $$\n",
    "\n",
    "* Add a regularization:\n",
    "\n",
    "$$\\sum_{i=1}^n L(y_i,f_m(x_i))+\\sum_{k=1}^m \\Omega(h_k),\\qquad \\Omega(h)= \\gamma |J|+\\frac{\\lambda}{2}\\sum_{j=1}^J w_j^2,$$\n",
    "\n",
    "where $f_m=h_1+\\dots+h_m$.\n",
    "\n",
    "* Use second order approximation (Newton method instead of gradient descent) for the approximate solution of the problem\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat h_k &\\in\\arg\\min_{h\\in\\mathscr H}\\left(\\sum_{i=1}^n L(y_i,\\hat f_{k-1}(x_i)+h(x_i))+\\Omega(h)\\right)\\\\\n",
    "&\\approx \\arg\\min_{h\\in\\mathscr H}\\left(\\sum_{i=1}^n \\alpha_i h(x_i)+\\frac{1}{2}\\beta_i h^2(x_i)+\\gamma |J|+\\frac{\\lambda}{2}\\sum_{j=1}^J w_j^2 \\right)\\\\\n",
    "&\\approx \\arg\\min_{h\\in\\mathscr H}\\left\\{\\sum_{j=1}^J \\left(\\left[\\sum_{i:x_i\\in R_j}\\alpha_i\\right] w_j+\\frac{1}{2}\\left[\\lambda+\\sum_{i:x_i\\in R_j}\\beta_i \\right]w_j^2 \\right)+\\gamma |J|\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "Minimize over $w$:\n",
    "   \n",
    "$$ w_j^*=-\\frac{\\sum_{i:x_i\\in R_j}\\alpha_i}{\\lambda+\\sum_{i:x_i\\in R_j}\\beta_i }$$\n",
    "\n",
    "The corresponding optimal value:\n",
    "\n",
    "$$ \\tilde {\\mathcal L}_k(h)=-\\frac{1}{2}\\sum_{j=1}^J \\frac{\\left(\\sum_{i:x_i\\in R_j}\\alpha_i\\right)^2}{\\lambda+\\sum_{i:x_i\\in R_j}\\beta_i }+\\gamma |J|$$\n",
    "\n",
    "$$ \\hat h_k \\in\\arg\\min_{h\\in\\mathscr H} \\tilde {\\mathcal L}_k(h)$$\n",
    "\n",
    "To find an approximately optimal tree use $\\tilde {\\mathcal L}_k(h)$ as an impurity measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2462fa97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723b1c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827872bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
